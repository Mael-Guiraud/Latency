%!TEX root = Manuscript.tex

\chapter*{Présentation de la thèse}
\label{chap:introfr}
\minitoc

Les travaux présentés dans cette thèse s'inscrivent dans le contexte du développement de la 5G, et sont plus particulièrement axés sur la réduction de la latence dans les réseaux cœur opérateur.
L'un des objectifs pour la 5G est de garantir une latence bout en bout la plus faible possible, afin de pouvoir développer des applications pour lesquelles le temps de réponse est critique (véhicules autonomes, industrie 4.0, etc.)
Le cas d'application que nous étudions est le C-RAN (pour Cloud Radio Access Network). Le but du C-RAN est de centraliser les unités de calculs situés aux pieds des antennes dans un ou plusieurs centres de calculs, afin de faciliter la maintenance et de réduire les couts d'exploitations. Les antennes envoient périodiquement des messages aux centres de calculs, qui calculent une réponse et l'envoient aux antennes avec la même fréquence. Le temps écoulé entre l'envoi d'un message par une antenne et la réception de sa réponse doit être inférieur à 3 ms, une contrainte imposée par le protocole de communication radio (HARQ). Ces messages envoyés sont très lourds, et utilisent donc beaucoup de bande passante dans les réseaux.

La gestion actuelle des réseaux, le multiplexage statistique consiste à dimensionner les liens de façon à ce que le flux moyen de message utilisant ces liens puisse passer en même temps. Quand trop de paquets doivent utiliser un lien en même temps, on parle de contention et une partie d'entre eux est mise en file d'attente, que nous appelons buffer de contention. Faire attendre les messages dans ces buffers de contention augmente la latence des paquets. Plus un réseau est chargé, plus la chance que les latences augmentent à cause de la contention. Les réseaux C-RAN envoyant une grande quantité de messages nécessitant une faible latence sont donc incompatibles avec la notion de multiplexage statistique.

Plusieurs groupes de travail proposent aujourd'hui des solutions techniques (présentées dans le chapitre~\ref{chap:context}) pour aider à contrôler la latence dans les réseaux. Avec ces solutions, les équipements du réseau sont capables de réserver une partie des ressources à un temps donné pour un paquet donné. Il faut toutefois calculer les temps auxquels les paquets doivent arriver dans les nœuds du réseau. Cette thèse se concentre sur le fait d'organiser les paquets, de façon à ce qu'ils n'entrent pas en collisions dans aucun des nœuds, dans le but de supprimer les buffers de contention. Se passer complètement des buffers de contention n'est pas toujours possible, notamment lorsque les réseaux sont composés de beaucoup de nœuds. Dans ce cas, l'objectif de nos travaux est de minimiser le temps passé par les paquets dans les files d'attentes. Il est important de souligner que dans ce cas-là les temps de contentions ne sont plus subis comme pour le multiplexage statistique, mais contrôlés.

Nous modélisons un réseau par un multigraphe orienté pondéré dont les sommets, représentent les points de contention entre messages dans le réseau. Deux messages rentrent en conflit s’ils doivent passer par un point de contention au même moment. Nous étudions dans un premier temps le problème sur des réseaux simples, mais communs avec seulement deux points de contention en série. Nous définissons le problème de décision consistant à choisir la date de passage de chaque message dans chacun de ces deux points de contention de façon à ce aucun message n'ai de conflit avec un autre dans le réseau. Ce problème ressemble à des problèmes classique d'ordonnancement mais l'envoi periodique de nos flux en fait un problème original et difficile.
 Nous prouvons dans le chapitre~\ref{chap:model} que le problème est $NP$-complet, même sur des graphes de faible largeur (et profondeur) d'arbre en réduisant des problèmes de coloration d'arcs et d'arêtes au nôtre.


Nous étudions dans le chapitre~\ref{chap:PAZL} le problème d'organiser des flux non-synchronisés dans le réseau sans aucun buffer, c’est-à-dire de trouver un temps de départ des messages au début de leur route de façon à ce qu'ils ne soient pas en conflit avec les autres messages, sans que ce temps de départ ne soit considéré comme du temps de contention. Les solutions de ce problème sont toutes optimales en termes de latence, car aucun temps de contention n'est ajouté aux messages. Nous décrivons des algorithmes de plus en plus évolués visant à optimiser une mesure commune sur l'impact d'ajouter un message à la solution partielle calculée. Ce algorithmes nous permettent de garantir qu'une solution au problème existe quand la charge du réseau est inférieure à $40\%$ (et même jusqu'a $61\%$ pour des messages de taille $1$). Nous proposons aussi un algorithme FPT (quand le problème est paramétré par le nombre de routes) qui nous permet de calculer la solution optimale quand le nombre de routes est inférieur à $20$. Les résultats montrent que le problème ne peut pas être résolu quand la charge du réseau est supérieure à $80\%$.
C'est pourquoi nous traitons dans le chapitre~\ref{chap:PALL} le problème d'organiser les flux avec un buffer sur la route, de façon à offrir un plus grand degré de liberté. Nous étudions plus particulièrement le problème de minimisation, c'est-à-dire, trouver une solution pour laquelle la plus grande latence est minimale. Nous proposons une approche en deux parties. Les temps d'envoi des messages sur le premier point de contention sont fixés en amont et nous résolvons le problème de choisir le temps d'attente de chaque message dans le second point de contention. Pour cela, nous décrivons un algorithme polynomial basé sur le problème d'ordonnancement classique de la littérature, adapté pour la périodicité. Nous proposons aussi un algorithme FPT basé sur le même principe, mais qui garanti de trouver la solution optimale. Nous montrons que nous sommes capables de trouver des solutions pour lesquelles la latence est minimale pour $99.9\%$ des instances dans des réseaux très chargés, et que nos travaux montrent des résultats excellents comparé au multiplexage statistique.

Dans le chapitre~\ref{chap:SPALL}, nous étudions le problème d'organiser des flux synchronisés sur tout type de DAG. Dans ce cas, tous les messages sont envoyés en même temps par les sources et nous nous permettons d'ajouter du temps de contention à chaque point de contention du réseau. Nous étudions le problème de minimiser la plus grande latence dans le réseau. Nous commençons par décrire des algorithmes gloutons qui trouvent une solution réalisable pour n'importe quelle charge, qui servent de point de départ aux algorithmes de recherche locales utilisés comme suit. Nous définissions une forme canonique du problème qui nous permet de proposer une notion de voisinage entre les solutions afin d'explorer l'ensemble de ces dernières. Nous étudions les performances des algorithmes de recherche d'optimum local (Descente, recherche tabou, recuit simulé) et nous proposons un algorithme Branch and Bound qui énumère l'ensemble des solutions sous forme canonique afin de trouver la solution optimale. Nous montrons expérimentalement que l'algorithme Branch and Bound est capable de trouver une solution optimale en un temps raisonnable pour $12$ routes, tandis que le recuit simulé permet de trouver des solutions bien meilleures que le multiplexage statistique pour n'importe quelle instance.

Nous étudions ensuite dans le chapitre~\ref{chap:BE} l'impact de nos algorithme d'ordonnancement de flux C-RAN lorsqu'ils partagent le réseau avec des flux Best-Effort. Nous proposons une méthode d'adaptation de nos algorithmes qui permet de lisser la charge des flux C-RAN tout au long de la période, sans augmenter leurs latences. Les résultats montrent qu'en organisant les flux de façon déterministe comme nous le faisons requiert d'utiliser un peu plus de bande passante pour réserver les ressources, la latence moyenne des flux Best-Effort est meilleure qu'avec le multiplexage statistique.

Toutes nos approches se basent sur des hypothèses techniques fortes : les flux doivent être parfaitement synchronisés, le réseau doit être intelligent et programmable. Le chapitre~\ref{chap:TSN} fait le point sur les standards récemment développés qui se rapprochent de nos hypothèses. Nous montrons aussi les limites de ces standards, et nous introduisons un équipement en phase de développement qui nous permettrais de réduire la latence dans les réseaux au temps physique de transmission.




\chapter*{Introduction}
\label{chap:introen}
\minitoc


