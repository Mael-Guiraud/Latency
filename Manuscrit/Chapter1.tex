%!TEX root = Manuscript.tex

\chapter*{Présentation de la thèse}
\label{chap:introfr}
\addcontentsline{toc}{chapter}{Présentation de la thèse}


Les travaux présentés dans cette thèse s'inscrivent dans le contexte du développement de la 5G, et sont plus particulièrement axés sur la réduction de la latence dans les réseaux cœur opérateur.
L'un des objectifs pour la 5G est de garantir une latence bout en bout la plus faible possible, afin de pouvoir développer des applications pour lesquelles le temps de réponse est critique (véhicules autonomes, industrie 4.0, etc.)
Le cas d'application que nous étudions est le C-RAN (pour Cloud Radio Access Network). Le but du C-RAN est de centraliser les unités de calcul situés aux pieds des antennes dans un ou plusieurs centres de calculs, afin de faciliter la maintenance et de réduire les couts d'exploitations. Les antennes envoient périodiquement des messages aux centres de calcul, qui calculent une réponse et l'envoient aux antennes avec la même période. Le temps écoulé entre l'envoi d'un message par une antenne et la réception de sa réponse doit être inférieur à $3$ ms, une contrainte imposée par le protocole de communication radio (HARQ). Ces messages envoyés sont très lourds, et utilisent donc beaucoup de bande passante dans les réseaux.

La méthode de gestion actuelle des réseaux, le multiplexage statistique, consiste à dimensionner chaque lien de façon à ce que le flux moyen de message utilisant un lien puisse emprunter ce lien sans contrainte. Quand trop de paquets doivent utiliser un lien en même temps, on parle de contention et une partie d'entre eux est mise dans une file d'attente, que nous appelons buffer de contention. Faire attendre les messages dans ces buffers de contention augmente la latence des paquets. Plus un réseau est chargé, plus il est probable d'avoir de hautes latences dues à la contention. Comme les réseaux C-RAN envoyent une grande quantité de messages nécessitant une garantie de faible latence, ils ne peuvent être gérés grâce au multiplexage statistique.

Plusieurs groupes de travail proposent aujourd'hui des solutions techniques (présentées dans le chapitre~\ref{chap:context}) pour aider à contrôler la latence dans les réseaux. Avec ces solutions, les équipements du réseau sont capables de réserver une partie des ressources à un temps donné pour un paquet donné. Il faut toutefois calculer les dates auxquels les paquets doivent arriver dans les nœuds du réseau. Cette thèse se concentre sur le fait d'organiser les paquets, de façon à ce qu'ils n'entrent en collision dans aucun des nœuds, afin de supprimer les buffers de contention. Se passer complètement des buffers de contention n'est pas toujours possible, notamment lorsque les réseaux sont composés de beaucoup de nœuds. Dans ce cas, l'objectif de nos travaux est de minimiser le temps passé par les paquets dans les files d'attentes. Il est important de souligner que dans ce cas-là que les temps de contentions ne sont plus subis comme pour le multiplexage statistique, mais contrôlés.

Nous modélisons un réseau par un multigraphe orienté pondéré dont les sommets, représentent les points de contention entre messages dans le réseau. Deux messages rentrent en conflit s’ils doivent passer par le même point de contention au même moment. Nous étudions dans un premier temps le problème sur des réseaux simples et courant, constitués de deux points de contention en série. Nous définissons le problème de décision consistant à choisir la date de passage de chaque message dans chacun de ces deux points de contention de façon à ce qu'aucun message n'ai de conflit avec un autre dans le réseau. Ce problème ressemble à des problèmes classique d'ordonnancement mais l'envoi periodique de nos flux en fait un problème original et difficile. Nous prouvons dans le chapitre~\ref{chap:model} que le problème est $NP$-complet, même sur des graphes orientés acycliques de faible degré ou de faible profondeur, par réduction de problèmes de coloration d'arcs ou de sommets.


Nous étudions dans le chapitre~\ref{chap:PAZL} le problème d'organiser des flux non-synchronisés dans le réseau sans aucun buffer, c’est-à-dire de trouver une temps de départ des messages au début de leur route de façon à ce qu'ils ne soient pas en conflit avec les autres messages, sans que ce temps de départ ne soit considéré comme du temps de contention. Les solutions de ce problème sont toutes optimales en termes de latence, car aucune latence n'est ajoutée aux messages à cause de la contention. Nous décrivons des algorithmes de plus en plus évolués visant à optimiser l'impact d'ajouter un message à la solution partielle calculée. Ces algorithmes nous permettent de garantir qu'une solution au problème existe quand la charge du réseau est inférieure à $40\%$ (et même jusqu'a $61\%$ pour des messages de taille $1$). Nous proposons aussi un algorithme FPT (quand le problème est paramétré par le nombre de routes) qui nous permet de calculer la solution optimale quand le nombre de routes est inférieur à $20$. Les résultats montrent que le problème ne peut pas être résolu quand la charge du réseau est supérieure à $80\%$.
C'est pourquoi nous traitons dans le chapitre~\ref{chap:PALL} le problème d'organiser les flux avec un buffer sur la route, de façon à offrir un plus grand degré de liberté. Nous étudions plus particulièrement le problème de minimisation, c'est-à-dire, trouver une solution pour laquelle le maximum des latences des routes est minimal. Nous proposons une approche en deux parties. Les temps d'envoi des messages sur le premier point de contention sont fixés en amont et nous résolvons le problème de choisir le temps d'attente de chaque message dans le second point de contention. Pour cela, nous décrivons un algorithme polynomial basé sur le problème d'ordonnancement classique de la littérature, adapté pour la périodicité. Nous proposons aussi un algorithme FPT basé sur le même principe, mais qui garantit de trouver la solution optimale. Nous montrons que nous sommes capables de trouver des solutions pour lesquelles la latence est minimale pour $99.9\%$ des instances dans des réseaux très chargés, et que nos méthodes donnent des résultats excellents comparées au multiplexage statistique.

Dans le chapitre~\ref{chap:SPALL}, nous étudions le problème d'organiser des flux synchronisés sur tout type de DAG. Dans ce cas, tous les messages sont envoyés en même temps par les sources et nous nous permettons de faire attendre les messages dans des buffers à chaque point de contention du réseau. Nous étudions le problème de minimiser la plus grande latence dans le réseau. Nous commençons par décrire des algorithmes gloutons qui trouvent une solution réalisable pour n'importe quelle charge, qui servent de point de départ aux algorithmes de recherche locale utilisés ensuite. Nous définissions une forme compacte du problème qui nous permet de proposer une notion de voisinage entre les solutions afin d'explorer l'ensemble de ces dernières. Nous étudions les performances des algorithmes de recherche d'optimum local (hill-climbing, recherche tabou, recuit simulé) et nous proposons un algorithme Branch and Bound qui énumère l'ensemble des solutions sous forme compacte, en faisaint  suffisemment de coupe pour trouver la solution optimale rapidement. Nous montrons expérimentalement que l'algorithme Branch and Bound est capable de trouver une solution optimale en un temps raisonnable pour $12$ routes, tandis que le recuit simulé permet de trouver des solutions bien meilleures que le multiplexage statistique pour n'importe quelle taille d'instance.

Nous étudions ensuite dans le chapitre~\ref{chap:BE} l'impact de nos algorithmes d'ordonnancement de flux C-RAN lorsqu'ils partagent le réseau avec des flux Best-Effort. Nous proposons une méthode d'adaptation de nos algorithmes qui permet de lisser la charge des flux C-RAN tout au long de la période, sans augmenter leurs latences. Les résultats montrent qu'en organisant les flux de façon déterministe comme nous le faisons requiert d'utiliser un peu plus de bande passante pour réserver les ressources, la latence moyenne des flux Best-Effort est meilleure qu'avec le multiplexage statistique. Nous montrons aussi le même genre de résultats dans un anneau optique ou l'ordonnancement des flux C-RAN est rendu trivial par les contraintes techniques de la conversion opto-electronique.

Toutes nos approches se basent sur des hypothèses techniques fortes: les flux doivent être parfaitement synchronisés, le réseau doit être intelligent et programmable. Le chapitre~\ref{chap:TSN} fait le point sur les standards récemment développés qui se rapprochent de nos hypothèses. Nous montrons aussi les limites de ces standards, et nous introduisons un équipement en phase de développement qui nous permettrais de réduire la latence dans les réseaux au temps physique de transmission.




\chapter*{Introduction (in English)}
\label{chap:introen}
\addcontentsline{toc}{chapter}{Introduction (in English)}


The work presented in this thesis fits into the development of 5G, and is more particularly focused on the reduction of latency in core operator networks.
One of the objectives for 5G is to guarantee low end-to-end latency, in order to be able to develop applications for which response time is critical (autonomous vehicles, industry 4.0, etc.).
The application case we are studying is C-RAN (for Cloud Radio Access Network). The goal of C-RAN is to centralize the computing units located at the foot of the antennas in one or more data-centers, in order to facilitate maintenance and reduce operating costs. The antennas periodically send messages to the data-centers, which calculate an answer and send it to the antennas with the same frequency. The time elapsed between the sending of a message by an antenna and the reception of its answer must be less than $3$ ms, a constraint imposed by the radio communication protocol (HARQ). These sent messages are very heavy, and therefore use a lot of bandwidth in networks.

Current networks management, called statistical multiplexing consists in dimensioning the links so that the average message flow using these links can use it at the same time. When too many packets have to use a link at the same time, we talk about contention and some of them are queued, in what we call contention buffer. Time spent by messages in these contention buffers increases packet latency. The more a network is loaded, the more the chance that latencies increase because of contention. In C-RAN networks, the antennas and data-centers send a large amount of messages requiring low latency. This use case is then incompatible with the notion of statistical multiplexing.

Several working groups are now proposing technical solutions (presented in chapter~\ref{chap:context}) in order to help to control latency in networks. With these solutions, network equipments are able to reserve part of the resources at a given time for a given packet. However, it is necessary to calculate the dates at which packets must arrive at the network nodes. This thesis focuses on organizing packets so that they do not collide in any of the nodes, in order to remove contention buffers. totally get rid of contention buffers is not always possible, especially when networks are composed of many nodes. In this case, the goal of our work is to minimize the time spent by packets in the queues. It is important to underline that in this case contention times are no longer undergone, as in statistical multiplexing, but controlled.

We model a network using a weighted directed multigraph whose vertices represent the contention points between messages in the network. Two messages conflict if they must pass through the same contention point at the same time. We first study the problem on simple but common networks with only two serial contention points. We define the decision problem of choosing the date at which each message through each of these two contention points so that no message has a conflict with another in the network. This problem looks like classical scheduling problems but the periodicity of our flows makes our problem original and difficult.
 We prove in chapter~\ref{chap:model} that the problem is $NP$-complete, even on graphs with small treewidth (and depth) by reducing arc and edge coloring problems to ours.


We study in chapter~\ref{chap:PAZL} the problem of organizing non-synchronized flows in the network without any buffer, i.e. finding a departure time for messages at the beginning of their route so that they do not conflict with other messages, without additional contention time due to this departure time. The solutions to this problem are all optimal in terms of latency, because no contention time is added to the messages. We describe increasingly advanced algorithms aimed at optimizing a common measure of the impact of adding a message to the partial solution. These algorithms allow us to ensure that a solution to the problem exists when the network load is less than $40\%$ (and even up to $61\%$ for messages of size $1$). We also propose an FPT algorithm (when the problem is parameterized by the number of routes) that allows us to calculate the optimal solution when the number of routes is less than $20$. The results show that the problem cannot be solved when the network load is higher than $80\%$.
This is why we deal in chapter~\ref{chap:PALL} with the problem of organizing the flows with one buffer on the route, so as to offer a greater degree of freedom. In particular, we study the problem of minimization, that is, finding a solution for which the highest latency is minimal. We propose a two-stage approach. The departure times of the messages are fixed upstream such that there is no conflict in the first contention point, and we solve the problem of choosing the waiting time for each message on the second contention point. To do so, we describe a polynomial algorithm based on the classical scheduling problem of the literature, adapted for periodicity. We also propose an FPT algorithm based on the same principle,that guarantees to find the optimal solution. We show that we are able to find solutions for which latency is minimal for $99.9\%$ of instances on highly loaded networks, and that our work shows excellent results compared to statistical multiplexing.

In chapter~\ref{chap:SPALL}, we study the problem of organizing synchronized flows on any type of DAG. In this case, all messages are sent at the same time by the sources and we allow ourselves to add contention time at each contention point of the network. We study the problem of minimizing the highest latency in the network. We start by describing greedy algorithms that find a valid solution for any load, which serve as a starting point for the local search algorithms used as follows. We define a canonical form of the problem that allows us to propose an efficient notion of neighborhood between the solutions in order to explore all of the solutions. We study the performances of the local search algorithms (Descent, taboo search, simulated annealing) and we propose a Branch and Bound algorithm that lists all the solutions in canonical form in order to find the optimal solution. We show experimentally that the Branch and Bound algorithm is able to find an optimal solution in a reasonable time for $12$ routes, while simulated annealing allows to find much better solutions than statistical multiplexing for any instance.

We then study in chapter~\ref{chap:BE} the impact of scheduling C-RAN flows when sharing the network with Best-Effort flows. We propose a method to adapt our algorithms that smooth the load of C-RAN flows throughout the period, without increasing their latencies. The results show that by organizing the flows in a deterministic way as we do, even if it requires a little more bandwidth to resources reservation, the average latency of the Best-Effort flows is better than with statistical multiplexing.

All our approaches are based on strong technical assumptions: the flows must be perfectly synchronized, the network must be intelligent and programmable. The chapter~\ref{chap:TSN} reviews the recently developed standards that are close to our assumptions. We also show the limits of these standards, and we introduce equipment under development that would allow us to reduce latency in networks to the physical transmission time.

